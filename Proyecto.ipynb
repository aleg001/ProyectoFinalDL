{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final de Deep Learning: Desenvolviendo el Sonido en la Universidad del Valle de Guatemala\n",
    "\n",
    "> Este trabajo se basa en distintos proyectos de separaci贸n de audio, como por ejemplo el proyecto [Music Source Separation](https://github.com/andabi/music-source-separation) desarrollado durante el [Jeju Machine Learning Camp 2017](http://mlcampjeju.kakao.com). Sin embargo, han servido como base y han sido extensamente modificados y mejorados como parte del proyecto final para la Universidad del Valle de Guatemala por Ale G贸mez, Michy Solano, Andrea Lam, Chris Garc铆a, Gabo Vicente y Rodri Barrera.\n",
    "\n",
    "## Introducci贸n \n",
    "\n",
    "La separaci贸n de fuentes musicales es una tarea esencial en el procesamiento de se帽ales de audio, que se centra en separar diferentes componentes de una canci贸n, como la voz y los instrumentos. Este proyecto busca mejorar la arquitectura y la eficacia del modelo inicial propuesto en el repositorio base, explorando t茅cnicas avanzadas en redes neuronales y procesamiento de se帽ales.\n",
    "\n",
    "\n",
    "### Comparativas con Herramientas Existentes:\n",
    "\n",
    "- Comparaci贸n de rendimiento con herramientas existentes como Splitter AI, validando las mejoras implementadas y proporcionando un benchmark sobre el estado del arte.\n",
    "\n",
    "## Evaluaci贸n y M茅tricas \n",
    "\n",
    "- Utilizaci贸n de m茅tricas est谩ndar en la tarea de separaci贸n de fuentes como SDR, SIR y SAR, adem谩s de otras m茅tricas relevantes como la precisi贸n y la recall en la detecci贸n de componentes vocales e instrumentales.\n",
    "- Documentaci贸n meticulosa de los resultados obtenidos, incluyendo visualizaciones de espectrogramas y comparativas cualitativas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base del modelo:\n",
    "\n",
    "- 3 capas RNN\n",
    "- 2 capas Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "\n",
    "Instrucciones de uso:\n",
    "\n",
    "Agregar paths correctamente en la secci贸n de \"Configuraci贸n\"\n",
    "Correr el notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Diff(object):\n",
    "    def __init__(self, v=0.0):\n",
    "        self.value = v\n",
    "        self.diff = 0.0\n",
    "\n",
    "    def update(self, v):\n",
    "        if self.value:\n",
    "            diff = v / self.value - 1\n",
    "            self.diff = diff\n",
    "        self.value = v\n",
    "\n",
    "\n",
    "def shape(tensor):\n",
    "    s = tensor.get_shape()\n",
    "    return tuple([s[i].value for i in range(0, len(s))])\n",
    "\n",
    "\n",
    "def pretty_list(list):\n",
    "    return \", \".join(list)\n",
    "\n",
    "\n",
    "def pretty_dict(dict):\n",
    "    return \"\\n\".join(\"{} : {}\".format(k, v) for k, v in dict.items())\n",
    "\n",
    "\n",
    "def closest_power_of_two(target):\n",
    "    if target > 1:\n",
    "        for i in range(1, int(target)):\n",
    "            if 2**i >= target:\n",
    "                pwr = 2**i\n",
    "                break\n",
    "        if abs(pwr - target) < abs(pwr / 2 - target):\n",
    "            return pwr\n",
    "        else:\n",
    "            return int(pwr / 2)\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "# Write the nd array to txtfile\n",
    "def nd_array_to_txt(filename, data):\n",
    "    path = filename + \".txt\"\n",
    "    file = open(path, \"w\")\n",
    "    with file as outfile:\n",
    "        # I'm writing a header here just for the sake of readability\n",
    "        # Any line starting with \"#\" will be ignored by numpy.loadtxt\n",
    "        outfile.write(\"# Array shape: {0}\\n\".format(data.shape))\n",
    "\n",
    "        # Iterating through a ndimensional array produces slices along\n",
    "        # the last axis. This is equivalent to data[i,:,:] in this case\n",
    "        for data_slice in data:\n",
    "\n",
    "            # The formatting string indicates that I'm writing out\n",
    "            # the values in left-justified columns 7 characters in width\n",
    "            # with 2 decimal places.\n",
    "            np.savetxt(outfile, data_slice, fmt=\"%-7.2f\")\n",
    "\n",
    "            # Writing out a break to indicate different slices...\n",
    "            outfile.write(\"# New slice\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuraci贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ModelConfig:\n",
    "    SR = 16000  # Sample Rate\n",
    "    L_FRAME = 1024  # default 1024\n",
    "    L_HOP = closest_power_of_two(L_FRAME / 4)\n",
    "    SEQ_LEN = 4\n",
    "    # For Melspectogram\n",
    "    N_MELS = 512\n",
    "    F_MIN = 0.0\n",
    "\n",
    "\n",
    "# Train\n",
    "class TrainConfig:\n",
    "    CASE = str(ModelConfig.SEQ_LEN) + \"frames_ikala\"\n",
    "    CKPT_PATH = \"checkpoints/\" + CASE\n",
    "    GRAPH_PATH = \"graphs/\" + CASE + \"/train\"\n",
    "    DATA_PATH = \"dataset/train/ikala\"\n",
    "    LR = 0.0001\n",
    "    FINAL_STEP = 100000\n",
    "    CKPT_STEP = 500\n",
    "    NUM_WAVFILE = 1\n",
    "    SECONDS = 8.192  # To get 512,512 in melspecto\n",
    "    RE_TRAIN = True\n",
    "    session_conf = tf.ConfigProto(\n",
    "        device_count={\"CPU\": 1, \"GPU\": 1},\n",
    "        gpu_options=tf.GPUOptions(\n",
    "            allow_growth=True, per_process_gpu_memory_fraction=0.25\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "class EvalConfig:\n",
    "    # CASE = '1frame'\n",
    "    # CASE = '4-frames-masking-layer'\n",
    "    CASE = str(ModelConfig.SEQ_LEN) + \"frames_ikala\"\n",
    "    CKPT_PATH = \"checkpoints/\" + CASE\n",
    "    GRAPH_PATH = \"graphs/\" + CASE + \"/eval\"\n",
    "    DATA_PATH = \"dataset/eval/kpop\"\n",
    "    # DATA_PATH = 'dataset/mir-1k/Wavfile'\n",
    "    # DATA_PATH = 'dataset/ikala'\n",
    "    GRIFFIN_LIM = False\n",
    "    GRIFFIN_LIM_ITER = 1000\n",
    "    NUM_EVAL = 9\n",
    "    SECONDS = 60\n",
    "    RE_EVAL = True\n",
    "    EVAL_METRIC = False\n",
    "    WRITE_RESULT = True\n",
    "    RESULT_PATH = \"results/\" + CASE\n",
    "    session_conf = tf.ConfigProto(\n",
    "        device_count={\"CPU\": 1, \"GPU\": 1},\n",
    "        gpu_options=tf.GPUOptions(allow_growth=True),\n",
    "        log_device_placement=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# !/usr/bin/env python\n",
    "\"\"\"\n",
    "By Dabi Ahn. andabi412@gmail.com.\n",
    "https://www.github.com/andabi\n",
    "\n",
    "Modificaciones por Grupo 5 - Proyecto Final Deep Learning\n",
    "UVG - 2023\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import GRUCell, MultiRNNCell\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, n_rnn_layer=3, hidden_size=256):\n",
    "\n",
    "        # Input, Output\n",
    "        self.x_mixed = tf.placeholder(\n",
    "            tf.float32, shape=(None, None, ModelConfig.L_FRAME // 2 + 1), name=\"x_mixed\"\n",
    "        )\n",
    "        self.y_src1 = tf.placeholder(\n",
    "            tf.float32, shape=(None, None, ModelConfig.L_FRAME // 2 + 1), name=\"y_src1\"\n",
    "        )\n",
    "        self.y_src2 = tf.placeholder(\n",
    "            tf.float32, shape=(None, None, ModelConfig.L_FRAME // 2 + 1), name=\"y_src2\"\n",
    "        )\n",
    "\n",
    "        # Network\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layer = n_rnn_layer\n",
    "        self.net = tf.make_template(\"net\", self._net)\n",
    "        self()\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.net()\n",
    "\n",
    "    def _net(self):\n",
    "        # RNN and dense layers\n",
    "        rnn_layer = MultiRNNCell(\n",
    "            [GRUCell(self.hidden_size) for _ in range(self.n_layer)]\n",
    "        )\n",
    "        output_rnn, rnn_state = tf.nn.dynamic_rnn(\n",
    "            rnn_layer, self.x_mixed, dtype=tf.float32\n",
    "        )\n",
    "        input_size = shape(self.x_mixed)[2]\n",
    "        y_hat_src1 = tf.layers.dense(\n",
    "            inputs=output_rnn,\n",
    "            units=input_size,\n",
    "            activation=tf.nn.relu,\n",
    "            name=\"y_hat_src1\",\n",
    "        )\n",
    "        y_hat_src2 = tf.layers.dense(\n",
    "            inputs=output_rnn,\n",
    "            units=input_size,\n",
    "            activation=tf.nn.relu,\n",
    "            name=\"y_hat_src2\",\n",
    "        )\n",
    "\n",
    "        # time-freq masking layer\n",
    "        y_tilde_src1 = (\n",
    "            y_hat_src1 / (y_hat_src1 + y_hat_src2 + np.finfo(float).eps) * self.x_mixed\n",
    "        )\n",
    "        y_tilde_src2 = (\n",
    "            y_hat_src2 / (y_hat_src1 + y_hat_src2 + np.finfo(float).eps) * self.x_mixed\n",
    "        )\n",
    "\n",
    "        return y_tilde_src1, y_tilde_src2\n",
    "\n",
    "    def loss(self):\n",
    "        pred_y_src1, pred_y_src2 = self()\n",
    "        return tf.reduce_mean(\n",
    "            tf.square(self.y_src1 - pred_y_src1) + tf.square(self.y_src2 - pred_y_src2),\n",
    "            name=\"loss\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    # shape = (batch_size, n_freq, n_frames) => (batch_size, n_frames, n_freq)\n",
    "    def spec_to_batch(src):\n",
    "        num_wavs, freq, n_frames = src.shape\n",
    "\n",
    "        # Padding\n",
    "        pad_len = 0\n",
    "        if n_frames % ModelConfig.SEQ_LEN > 0:\n",
    "            pad_len = ModelConfig.SEQ_LEN - (n_frames % ModelConfig.SEQ_LEN)\n",
    "        pad_width = ((0, 0), (0, 0), (0, pad_len))\n",
    "        padded_src = np.pad(\n",
    "            src, pad_width=pad_width, mode=\"constant\", constant_values=0\n",
    "        )\n",
    "\n",
    "        assert padded_src.shape[-1] % ModelConfig.SEQ_LEN == 0\n",
    "\n",
    "        batch = np.reshape(\n",
    "            padded_src.transpose(0, 2, 1), (-1, ModelConfig.SEQ_LEN, freq)\n",
    "        )\n",
    "        return batch, padded_src\n",
    "\n",
    "    @staticmethod\n",
    "    def batch_to_spec(src, num_wav):\n",
    "        # shape = (batch_size, n_frames, n_freq) => (batch_size, n_freq, n_frames)\n",
    "        batch_size, seq_len, freq = src.shape\n",
    "        src = np.reshape(src, (num_wav, -1, freq))\n",
    "        src = src.transpose(0, 2, 1)\n",
    "        return src\n",
    "\n",
    "    @staticmethod\n",
    "    def load_state(sess, ckpt_path):\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname(ckpt_path + \"/checkpoint\"))\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            tf.train.Saver().restore(sess, ckpt.model_checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "# Batch considered\n",
    "def get_random_wav(filenames, sec, sr=ModelConfig.SR):\n",
    "    # load wav -> pad if necessary to fit sr*sec -> get random samples with len = sr*sec -> map = do this for all in filenames -> put in np.array\n",
    "    src1_src2 = np.array(\n",
    "        list(\n",
    "            map(\n",
    "                lambda f: _sample_range(\n",
    "                    _pad_wav(librosa.load(f, sr=sr, mono=False)[0], sr, sec), sr, sec\n",
    "                ),\n",
    "                filenames,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    mixed = np.array(list(map(lambda f: librosa.to_mono(f), src1_src2)))\n",
    "    src1, src2 = src1_src2[:, 0], src1_src2[:, 1]\n",
    "    return mixed, src1, src2\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def to_spectrogram(wav, len_frame=ModelConfig.L_FRAME, len_hop=ModelConfig.L_HOP):\n",
    "    return np.array(\n",
    "        list(map(lambda w: librosa.stft(w, n_fft=len_frame, hop_length=len_hop), wav))\n",
    "    )\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def to_wav(mag, phase, len_hop=ModelConfig.L_HOP):\n",
    "    stft_matrix = get_stft_matrix(mag, phase)\n",
    "    return np.array(\n",
    "        list(map(lambda s: librosa.istft(s, hop_length=len_hop), stft_matrix))\n",
    "    )\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def to_wav_from_spec(stft_maxrix, len_hop=ModelConfig.L_HOP):\n",
    "    return np.array(\n",
    "        list(map(lambda s: librosa.istft(s, hop_length=len_hop), stft_maxrix))\n",
    "    )\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def to_wav_mag_only(\n",
    "    mag,\n",
    "    init_phase,\n",
    "    len_frame=ModelConfig.L_FRAME,\n",
    "    len_hop=ModelConfig.L_HOP,\n",
    "    num_iters=50,\n",
    "):\n",
    "    # return np.array(list(map(lambda m_p: griffin_lim(m, len_frame, len_hop, num_iters=num_iters, phase_angle=p)[0], list(zip(mag, init_phase))[1])))\n",
    "    return np.array(\n",
    "        list(\n",
    "            map(\n",
    "                lambda m: lambda p: griffin_lim(\n",
    "                    m, len_frame, len_hop, num_iters=num_iters, phase_angle=p\n",
    "                ),\n",
    "                list(zip(mag, init_phase))[1],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def get_magnitude(stft_matrixes):\n",
    "    return np.abs(stft_matrixes)\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def get_phase(stft_maxtrixes):\n",
    "    return np.angle(stft_maxtrixes)\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def get_stft_matrix(magnitudes, phases):\n",
    "    return magnitudes * np.exp(1.0j * phases)\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def soft_time_freq_mask(target_src, remaining_src):\n",
    "    mask = np.abs(target_src) / (\n",
    "        np.abs(target_src) + np.abs(remaining_src) + np.finfo(float).eps\n",
    "    )\n",
    "    return mask\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def hard_time_freq_mask(target_src, remaining_src):\n",
    "    mask = np.where(target_src > remaining_src, 1.0, 0.0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def write_wav(data, path, sr=ModelConfig.SR, format=\"wav\", subtype=\"PCM_16\"):\n",
    "    sf.write(\"{}.wav\".format(path), data, sr, format=format, subtype=subtype)\n",
    "\n",
    "\n",
    "def griffin_lim(mag, len_frame, len_hop, num_iters, phase_angle=None, length=None):\n",
    "    assert num_iters > 0\n",
    "    if phase_angle is None:\n",
    "        phase_angle = np.pi * np.random.rand(*mag.shape)\n",
    "    spec = get_stft_matrix(mag, phase_angle)\n",
    "    for i in range(num_iters):\n",
    "        wav = librosa.istft(\n",
    "            spec, win_length=len_frame, hop_length=len_hop, length=length\n",
    "        )\n",
    "        if i != num_iters - 1:\n",
    "            spec = librosa.stft(\n",
    "                wav, n_fft=len_frame, win_length=len_frame, hop_length=len_hop\n",
    "            )\n",
    "            _, phase = librosa.magphase(spec)\n",
    "            phase_angle = np.angle(phase)\n",
    "            spec = get_stft_matrix(mag, phase_angle)\n",
    "    return wav\n",
    "\n",
    "\n",
    "def _pad_wav(wav, sr, duration):\n",
    "    assert wav.ndim <= 2\n",
    "\n",
    "    n_samples = int(sr * duration)\n",
    "    pad_len = np.maximum(0, n_samples - wav.shape[-1])\n",
    "    if wav.ndim == 1:\n",
    "        pad_width = (0, pad_len)\n",
    "    else:\n",
    "        pad_width = ((0, 0), (0, pad_len))\n",
    "    wav = np.pad(wav, pad_width=pad_width, mode=\"constant\", constant_values=0)\n",
    "\n",
    "    return wav\n",
    "\n",
    "\n",
    "def _sample_range(wav, sr, duration):\n",
    "    assert wav.ndim <= 2\n",
    "\n",
    "    target_len = int(sr * duration)\n",
    "    wav_len = wav.shape[-1]\n",
    "    start = np.random.choice(range(np.maximum(1, wav_len - target_len)), 1)[0]\n",
    "    end = start + target_len\n",
    "    if wav.ndim == 1:\n",
    "        wav = wav[start:end]\n",
    "    else:\n",
    "        wav = wav[:, start:end]\n",
    "    return wav\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from os import walk\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def next_wavs(self, sec, size=1):\n",
    "        wavfiles = []\n",
    "        for (root, dirs, files) in walk(self.path):\n",
    "            wavfiles.extend(\n",
    "                [\"{}/{}\".format(root, f) for f in files if f.endswith(\".wav\")]\n",
    "            )\n",
    "        wavfiles = random.sample(wavfiles, size)\n",
    "        mixed, src1, src2 = get_random_wav(wavfiles, sec, ModelConfig.SR)\n",
    "        return mixed, src1, src2, wavfiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib as plt\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Model\n",
    "    model = Model()\n",
    "\n",
    "    # Loss, Optimizer\n",
    "    global_step = tf.Variable(\n",
    "        0, dtype=tf.int32, trainable=False, name=\"global_step\")\n",
    "    loss_fn = model.loss()\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=TrainConfig.LR).minimize(\n",
    "        loss_fn, global_step=global_step\n",
    "    )\n",
    "\n",
    "    # Summaries\n",
    "    summary_op = summaries(model, loss_fn)\n",
    "\n",
    "    with tf.Session(config=TrainConfig.session_conf) as sess:\n",
    "\n",
    "        # Initialized, Load state\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        model.load_state(sess, TrainConfig.CKPT_PATH)\n",
    "\n",
    "        writer = tf.summary.FileWriter(TrainConfig.GRAPH_PATH, sess.graph)\n",
    "\n",
    "        # Input source\n",
    "        data = Data(TrainConfig.DATA_PATH)\n",
    "\n",
    "        loss = Diff()\n",
    "        for step in range(\n",
    "            global_step.eval(), TrainConfig.FINAL_STEP\n",
    "        ):  # changed xrange to range for py3\n",
    "            mixed_wav, src1_wav, src2_wav, _ = data.next_wavs(\n",
    "                TrainConfig.SECONDS, TrainConfig.NUM_WAVFILE\n",
    "            )\n",
    "\n",
    "            mixed_spec = to_spectrogram(mixed_wav)\n",
    "            mixed_mag = get_magnitude(mixed_spec)\n",
    "\n",
    "            src1_spec, src2_spec = to_spectrogram(\n",
    "                src1_wav), to_spectrogram(src2_wav)\n",
    "            src1_mag, src2_mag = get_magnitude(\n",
    "                src1_spec), get_magnitude(src2_spec)\n",
    "\n",
    "            src1_batch, _ = model.spec_to_batch(src1_mag)\n",
    "            src2_batch, _ = model.spec_to_batch(src2_mag)\n",
    "            mixed_batch, _ = model.spec_to_batch(mixed_mag)\n",
    "\n",
    "            l, _, summary = sess.run(\n",
    "                [loss_fn, optimizer, summary_op],\n",
    "                feed_dict={\n",
    "                    model.x_mixed: mixed_batch,\n",
    "                    model.y_src1: src1_batch,\n",
    "                    model.y_src2: src2_batch,\n",
    "                },\n",
    "            )\n",
    "\n",
    "            loss.update(l)\n",
    "            print(\n",
    "                \"step-{}\\td_loss={:2.2f}\\tloss={}\".format(\n",
    "                    step, loss.diff * 100, loss.value\n",
    "                )\n",
    "            )\n",
    "\n",
    "            writer.add_summary(summary, global_step=step)\n",
    "\n",
    "            # Save state\n",
    "            if step % TrainConfig.CKPT_STEP == 0:\n",
    "                tf.train.Saver().save(\n",
    "                    sess, TrainConfig.CKPT_PATH + \"/checkpoint\", global_step=step\n",
    "                )\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "def summaries(model, loss):\n",
    "    for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "        tf.summary.histogram(v.name, v)\n",
    "        tf.summary.histogram(\"grad/\" + v.name, tf.gradients(loss, v))\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.histogram(\"x_mixed\", model.x_mixed)\n",
    "    tf.summary.histogram(\"y_src1\", model.y_src1)\n",
    "    tf.summary.histogram(\"y_src2\", model.y_src1)\n",
    "    return tf.summary.merge_all()\n",
    "\n",
    "\n",
    "def setup_path():\n",
    "    if TrainConfig.RE_TRAIN:\n",
    "        if os.path.exists(TrainConfig.CKPT_PATH):\n",
    "            shutil.rmtree(TrainConfig.CKPT_PATH)\n",
    "        if os.path.exists(TrainConfig.GRAPH_PATH):\n",
    "            shutil.rmtree(TrainConfig.GRAPH_PATH)\n",
    "    if not os.path.exists(TrainConfig.CKPT_PATH):\n",
    "        os.makedirs(TrainConfig.CKPT_PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_path()\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluaci贸n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from mir_eval.separation import bss_eval_sources\n",
    "\n",
    "\n",
    "\n",
    "def eval():\n",
    "    # Model\n",
    "    model = Model()\n",
    "    global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name=\"global_step\")\n",
    "\n",
    "    with tf.Session(config=EvalConfig.session_conf) as sess:\n",
    "\n",
    "        # Initialized, Load state\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        model.load_state(sess, EvalConfig.CKPT_PATH)\n",
    "\n",
    "        writer = tf.summary.FileWriter(EvalConfig.GRAPH_PATH, sess.graph)\n",
    "\n",
    "        data = Data(EvalConfig.DATA_PATH)\n",
    "        mixed_wav, src1_wav, src2_wav, wavfiles = data.next_wavs(\n",
    "            EvalConfig.SECONDS, EvalConfig.NUM_EVAL\n",
    "        )\n",
    "\n",
    "        mixed_spec = to_spectrogram(mixed_wav)\n",
    "        mixed_mag = get_magnitude(mixed_spec)\n",
    "        mixed_batch, padded_mixed_mag = model.spec_to_batch(mixed_mag)\n",
    "        mixed_phase = get_phase(mixed_spec)\n",
    "\n",
    "        assert np.all(\n",
    "            np.equal(\n",
    "                model.batch_to_spec(mixed_batch, EvalConfig.NUM_EVAL), padded_mixed_mag\n",
    "            )\n",
    "        )\n",
    "\n",
    "        (pred_src1_mag, pred_src2_mag) = sess.run(\n",
    "            model(), feed_dict={model.x_mixed: mixed_batch}\n",
    "        )\n",
    "\n",
    "        seq_len = mixed_phase.shape[-1]\n",
    "        pred_src1_mag = model.batch_to_spec(pred_src1_mag, EvalConfig.NUM_EVAL)[\n",
    "            :, :, :seq_len\n",
    "        ]\n",
    "        pred_src2_mag = model.batch_to_spec(pred_src2_mag, EvalConfig.NUM_EVAL)[\n",
    "            :, :, :seq_len\n",
    "        ]\n",
    "\n",
    "        # Time-frequency masking\n",
    "        mask_src1 = soft_time_freq_mask(pred_src1_mag, pred_src2_mag)\n",
    "        # mask_src1 = hard_time_freq_mask(pred_src1_mag, pred_src2_mag)\n",
    "        mask_src2 = 1.0 - mask_src1\n",
    "        pred_src1_mag = mixed_mag * mask_src1\n",
    "        pred_src2_mag = mixed_mag * mask_src2\n",
    "\n",
    "        # (magnitude, phase) -> spectrogram -> wav\n",
    "        if EvalConfig.GRIFFIN_LIM:\n",
    "            pred_src1_wav = to_wav_mag_only(\n",
    "                pred_src1_mag,\n",
    "                init_phase=mixed_phase,\n",
    "                num_iters=EvalConfig.GRIFFIN_LIM_ITER,\n",
    "            )\n",
    "            pred_src2_wav = to_wav_mag_only(\n",
    "                pred_src2_mag,\n",
    "                init_phase=mixed_phase,\n",
    "                num_iters=EvalConfig.GRIFFIN_LIM_ITER,\n",
    "            )\n",
    "        else:\n",
    "            pred_src1_wav = to_wav(pred_src1_mag, mixed_phase)\n",
    "            pred_src2_wav = to_wav(pred_src2_mag, mixed_phase)\n",
    "\n",
    "        # Write the result\n",
    "        tf.summary.audio(\n",
    "            \"GT_mixed\", mixed_wav, ModelConfig.SR, max_outputs=EvalConfig.NUM_EVAL\n",
    "        )\n",
    "        tf.summary.audio(\n",
    "            \"Pred_music\", pred_src1_wav, ModelConfig.SR, max_outputs=EvalConfig.NUM_EVAL\n",
    "        )\n",
    "        tf.summary.audio(\n",
    "            \"Pred_vocal\", pred_src2_wav, ModelConfig.SR, max_outputs=EvalConfig.NUM_EVAL\n",
    "        )\n",
    "\n",
    "        if EvalConfig.EVAL_METRIC:\n",
    "            # Compute BSS metrics\n",
    "            gnsdr, gsir, gsar = bss_eval_global(\n",
    "                mixed_wav, src1_wav, src2_wav, pred_src1_wav, pred_src2_wav\n",
    "            )\n",
    "\n",
    "            # Write the score of BSS metrics\n",
    "            tf.summary.scalar(\"GNSDR_music\", gnsdr[0])\n",
    "            tf.summary.scalar(\"GSIR_music\", gsir[0])\n",
    "            tf.summary.scalar(\"GSAR_music\", gsar[0])\n",
    "            tf.summary.scalar(\"GNSDR_vocal\", gnsdr[1])\n",
    "            tf.summary.scalar(\"GSIR_vocal\", gsir[1])\n",
    "            tf.summary.scalar(\"GSAR_vocal\", gsar[1])\n",
    "\n",
    "        if EvalConfig.WRITE_RESULT:\n",
    "            # Write the result\n",
    "            for i in range(len(wavfiles)):\n",
    "                name = wavfiles[i].replace(\"/\", \"-\").replace(\".wav\", \"\")\n",
    "                write_wav(\n",
    "                    mixed_wav[i],\n",
    "                    \"{}/{}-{}\".format(EvalConfig.RESULT_PATH, name, \"original\"),\n",
    "                )\n",
    "                write_wav(\n",
    "                    pred_src1_wav[i],\n",
    "                    \"{}/{}-{}\".format(EvalConfig.RESULT_PATH, name, \"music\"),\n",
    "                )\n",
    "                write_wav(\n",
    "                    pred_src2_wav[i],\n",
    "                    \"{}/{}-{}\".format(EvalConfig.RESULT_PATH, name, \"voice\"),\n",
    "                )\n",
    "\n",
    "        writer.add_summary(\n",
    "            sess.run(tf.summary.merge_all()), global_step=global_step.eval()\n",
    "        )\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "def bss_eval_global(mixed_wav, src1_wav, src2_wav, pred_src1_wav, pred_src2_wav):\n",
    "    len_cropped = pred_src1_wav.shape[-1]\n",
    "    src1_wav = src1_wav[:, :len_cropped]\n",
    "    src2_wav = src2_wav[:, :len_cropped]\n",
    "    mixed_wav = mixed_wav[:, :len_cropped]\n",
    "    gnsdr = gsir = gsar = np.zeros(2)\n",
    "    total_len = 0\n",
    "    for i in range(EvalConfig.NUM_EVAL):\n",
    "        sdr, sir, sar, _ = bss_eval_sources(\n",
    "            np.array([src1_wav[i], src2_wav[i]]),\n",
    "            np.array([pred_src1_wav[i], pred_src2_wav[i]]),\n",
    "            False,\n",
    "        )\n",
    "        sdr_mixed, _, _, _ = bss_eval_sources(\n",
    "            np.array([src1_wav[i], src2_wav[i]]),\n",
    "            np.array([mixed_wav[i], mixed_wav[i]]),\n",
    "            False,\n",
    "        )\n",
    "        nsdr = sdr - sdr_mixed\n",
    "        gnsdr += len_cropped * nsdr\n",
    "        gsir += len_cropped * sir\n",
    "        gsar += len_cropped * sar\n",
    "        total_len += len_cropped\n",
    "    gnsdr = gnsdr / total_len\n",
    "    gsir = gsir / total_len\n",
    "    gsar = gsar / total_len\n",
    "    return gnsdr, gsir, gsar\n",
    "\n",
    "\n",
    "def setup_path():\n",
    "    if EvalConfig.RE_EVAL:\n",
    "        if os.path.exists(EvalConfig.GRAPH_PATH):\n",
    "            shutil.rmtree(EvalConfig.GRAPH_PATH)\n",
    "        if os.path.exists(EvalConfig.RESULT_PATH):\n",
    "            shutil.rmtree(EvalConfig.RESULT_PATH)\n",
    "\n",
    "    if not os.path.exists(EvalConfig.RESULT_PATH):\n",
    "        os.makedirs(EvalConfig.RESULT_PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_path()\n",
    "    eval()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
