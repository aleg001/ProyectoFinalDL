{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final de Deep Learning: Desenvolviendo el Sonido en la Universidad del Valle de Guatemala\n",
    "\n",
    "> Este trabajo se basa en distintos proyectos de separación de audio, como por ejemplo el proyecto [Music Source Separation](https://github.com/andabi/music-source-separation) desarrollado durante el [Jeju Machine Learning Camp 2017](http://mlcampjeju.kakao.com). Sin embargo, han servido como base y han sido extensamente modificados y mejorados como parte del proyecto final para la Universidad del Valle de Guatemala por Ale Gómez, Michy Solano, Andrea Lam, Chris García, Gabo Vicente y Rodri Barrera.\n",
    "\n",
    "## Introducción 🎵\n",
    "\n",
    "La separación de fuentes musicales es una tarea esencial en el procesamiento de señales de audio, que se centra en separar diferentes componentes de una canción, como la voz y los instrumentos. Este proyecto busca mejorar la arquitectura y la eficacia del modelo inicial propuesto en el repositorio base, explorando técnicas avanzadas en redes neuronales y procesamiento de señales.\n",
    "\n",
    "\n",
    "### Comparativas con Herramientas Existentes:\n",
    "\n",
    "- Comparación de rendimiento con herramientas existentes como Splitter AI, validando las mejoras implementadas y proporcionando un benchmark sobre el estado del arte.\n",
    "\n",
    "## Evaluación y Métricas 📊\n",
    "\n",
    "- Utilización de métricas estándar en la tarea de separación de fuentes como SDR, SIR y SAR, además de otras métricas relevantes como la precisión y la recall en la detección de componentes vocales e instrumentales.\n",
    "- Documentación meticulosa de los resultados obtenidos, incluyendo visualizaciones de espectrogramas y comparativas cualitativas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base del modelo:\n",
    "\n",
    "- 3 capas RNN\n",
    "- 2 capas Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "\n",
    "Instrucciones de uso:\n",
    "\n",
    "Agregar paths correctamente en la sección de \"Configuración\"\n",
    "Correr el notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Diff(object):\n",
    "    def __init__(self, v=0.0):\n",
    "        self.value = v\n",
    "        self.diff = 0.0\n",
    "\n",
    "    def update(self, v):\n",
    "        if self.value:\n",
    "            diff = v / self.value - 1\n",
    "            self.diff = diff\n",
    "        self.value = v\n",
    "\n",
    "\n",
    "def shape(tensor):\n",
    "    s = tensor.get_shape()\n",
    "    return tuple([s[i].value for i in range(0, len(s))])\n",
    "\n",
    "\n",
    "def pretty_list(list):\n",
    "    return \", \".join(list)\n",
    "\n",
    "\n",
    "def pretty_dict(dict):\n",
    "    return \"\\n\".join(\"{} : {}\".format(k, v) for k, v in dict.items())\n",
    "\n",
    "\n",
    "def closest_power_of_two(target):\n",
    "    if target > 1:\n",
    "        for i in range(1, int(target)):\n",
    "            if 2**i >= target:\n",
    "                pwr = 2**i\n",
    "                break\n",
    "        if abs(pwr - target) < abs(pwr / 2 - target):\n",
    "            return pwr\n",
    "        else:\n",
    "            return int(pwr / 2)\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "# Write the nd array to txtfile\n",
    "def nd_array_to_txt(filename, data):\n",
    "    path = filename + \".txt\"\n",
    "    file = open(path, \"w\")\n",
    "    with file as outfile:\n",
    "        # I'm writing a header here just for the sake of readability\n",
    "        # Any line starting with \"#\" will be ignored by numpy.loadtxt\n",
    "        outfile.write(\"# Array shape: {0}\\n\".format(data.shape))\n",
    "\n",
    "        # Iterating through a ndimensional array produces slices along\n",
    "        # the last axis. This is equivalent to data[i,:,:] in this case\n",
    "        for data_slice in data:\n",
    "\n",
    "            # The formatting string indicates that I'm writing out\n",
    "            # the values in left-justified columns 7 characters in width\n",
    "            # with 2 decimal places.\n",
    "            np.savetxt(outfile, data_slice, fmt=\"%-7.2f\")\n",
    "\n",
    "            # Writing out a break to indicate different slices...\n",
    "            outfile.write(\"# New slice\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ModelConfig:\n",
    "    SR = 16000  # Sample Rate\n",
    "    L_FRAME = 1024  # default 1024\n",
    "    L_HOP = closest_power_of_two(L_FRAME / 4)\n",
    "    SEQ_LEN = 4\n",
    "    # For Melspectogram\n",
    "    N_MELS = 512\n",
    "    F_MIN = 0.0\n",
    "\n",
    "\n",
    "# Train\n",
    "class TrainConfig:\n",
    "    CASE = str(ModelConfig.SEQ_LEN) + \"frames_ikala\"\n",
    "    CKPT_PATH = \"checkpoints/\" + CASE\n",
    "    GRAPH_PATH = \"graphs/\" + CASE + \"/train\"\n",
    "    DATA_PATH = \"dataset/train/ikala\"\n",
    "    LR = 0.0001\n",
    "    FINAL_STEP = 100000\n",
    "    CKPT_STEP = 500\n",
    "    NUM_WAVFILE = 1\n",
    "    SECONDS = 8.192  # To get 512,512 in melspecto\n",
    "    RE_TRAIN = True\n",
    "    session_conf = tf.ConfigProto(\n",
    "        device_count={\"CPU\": 1, \"GPU\": 1},\n",
    "        gpu_options=tf.GPUOptions(\n",
    "            allow_growth=True, per_process_gpu_memory_fraction=0.25\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "class EvalConfig:\n",
    "    # CASE = '1frame'\n",
    "    # CASE = '4-frames-masking-layer'\n",
    "    CASE = str(ModelConfig.SEQ_LEN) + \"frames_ikala\"\n",
    "    CKPT_PATH = \"checkpoints/\" + CASE\n",
    "    GRAPH_PATH = \"graphs/\" + CASE + \"/eval\"\n",
    "    DATA_PATH = \"dataset/eval/kpop\"\n",
    "    # DATA_PATH = 'dataset/mir-1k/Wavfile'\n",
    "    # DATA_PATH = 'dataset/ikala'\n",
    "    GRIFFIN_LIM = False\n",
    "    GRIFFIN_LIM_ITER = 1000\n",
    "    NUM_EVAL = 9\n",
    "    SECONDS = 60\n",
    "    RE_EVAL = True\n",
    "    EVAL_METRIC = False\n",
    "    WRITE_RESULT = True\n",
    "    RESULT_PATH = \"results/\" + CASE\n",
    "    session_conf = tf.ConfigProto(\n",
    "        device_count={\"CPU\": 1, \"GPU\": 1},\n",
    "        gpu_options=tf.GPUOptions(allow_growth=True),\n",
    "        log_device_placement=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# !/usr/bin/env python\n",
    "\"\"\"\n",
    "By Dabi Ahn. andabi412@gmail.com.\n",
    "https://www.github.com/andabi\n",
    "\n",
    "Modificaciones por Grupo 5 - Proyecto Final Deep Learning\n",
    "UVG - 2023\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import GRUCell, MultiRNNCell\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, n_rnn_layer=3, hidden_size=256):\n",
    "\n",
    "        # Input, Output\n",
    "        self.x_mixed = tf.placeholder(\n",
    "            tf.float32, shape=(None, None, ModelConfig.L_FRAME // 2 + 1), name=\"x_mixed\"\n",
    "        )\n",
    "        self.y_src1 = tf.placeholder(\n",
    "            tf.float32, shape=(None, None, ModelConfig.L_FRAME // 2 + 1), name=\"y_src1\"\n",
    "        )\n",
    "        self.y_src2 = tf.placeholder(\n",
    "            tf.float32, shape=(None, None, ModelConfig.L_FRAME // 2 + 1), name=\"y_src2\"\n",
    "        )\n",
    "\n",
    "        # Network\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layer = n_rnn_layer\n",
    "        self.net = tf.make_template(\"net\", self._net)\n",
    "        self()\n",
    "\n",
    "    def __call__(self):\n",
    "        return self.net()\n",
    "\n",
    "    def _net(self):\n",
    "        # RNN and dense layers\n",
    "        rnn_layer = MultiRNNCell(\n",
    "            [GRUCell(self.hidden_size) for _ in range(self.n_layer)]\n",
    "        )\n",
    "        output_rnn, rnn_state = tf.nn.dynamic_rnn(\n",
    "            rnn_layer, self.x_mixed, dtype=tf.float32\n",
    "        )\n",
    "        input_size = shape(self.x_mixed)[2]\n",
    "        y_hat_src1 = tf.layers.dense(\n",
    "            inputs=output_rnn,\n",
    "            units=input_size,\n",
    "            activation=tf.nn.relu,\n",
    "            name=\"y_hat_src1\",\n",
    "        )\n",
    "        y_hat_src2 = tf.layers.dense(\n",
    "            inputs=output_rnn,\n",
    "            units=input_size,\n",
    "            activation=tf.nn.relu,\n",
    "            name=\"y_hat_src2\",\n",
    "        )\n",
    "\n",
    "        # time-freq masking layer\n",
    "        y_tilde_src1 = (\n",
    "            y_hat_src1 / (y_hat_src1 + y_hat_src2 + np.finfo(float).eps) * self.x_mixed\n",
    "        )\n",
    "        y_tilde_src2 = (\n",
    "            y_hat_src2 / (y_hat_src1 + y_hat_src2 + np.finfo(float).eps) * self.x_mixed\n",
    "        )\n",
    "\n",
    "        return y_tilde_src1, y_tilde_src2\n",
    "\n",
    "    def loss(self):\n",
    "        pred_y_src1, pred_y_src2 = self()\n",
    "        return tf.reduce_mean(\n",
    "            tf.square(self.y_src1 - pred_y_src1) + tf.square(self.y_src2 - pred_y_src2),\n",
    "            name=\"loss\",\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    # shape = (batch_size, n_freq, n_frames) => (batch_size, n_frames, n_freq)\n",
    "    def spec_to_batch(src):\n",
    "        num_wavs, freq, n_frames = src.shape\n",
    "\n",
    "        # Padding\n",
    "        pad_len = 0\n",
    "        if n_frames % ModelConfig.SEQ_LEN > 0:\n",
    "            pad_len = ModelConfig.SEQ_LEN - (n_frames % ModelConfig.SEQ_LEN)\n",
    "        pad_width = ((0, 0), (0, 0), (0, pad_len))\n",
    "        padded_src = np.pad(\n",
    "            src, pad_width=pad_width, mode=\"constant\", constant_values=0\n",
    "        )\n",
    "\n",
    "        assert padded_src.shape[-1] % ModelConfig.SEQ_LEN == 0\n",
    "\n",
    "        batch = np.reshape(\n",
    "            padded_src.transpose(0, 2, 1), (-1, ModelConfig.SEQ_LEN, freq)\n",
    "        )\n",
    "        return batch, padded_src\n",
    "\n",
    "    @staticmethod\n",
    "    def batch_to_spec(src, num_wav):\n",
    "        # shape = (batch_size, n_frames, n_freq) => (batch_size, n_freq, n_frames)\n",
    "        batch_size, seq_len, freq = src.shape\n",
    "        src = np.reshape(src, (num_wav, -1, freq))\n",
    "        src = src.transpose(0, 2, 1)\n",
    "        return src\n",
    "\n",
    "    @staticmethod\n",
    "    def load_state(sess, ckpt_path):\n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname(ckpt_path + \"/checkpoint\"))\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            tf.train.Saver().restore(sess, ckpt.model_checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "# Batch considered\n",
    "def get_random_wav(filenames, sec, sr=ModelConfig.SR):\n",
    "    # load wav -> pad if necessary to fit sr*sec -> get random samples with len = sr*sec -> map = do this for all in filenames -> put in np.array\n",
    "    src1_src2 = np.array(\n",
    "        list(\n",
    "            map(\n",
    "                lambda f: _sample_range(\n",
    "                    _pad_wav(librosa.load(f, sr=sr, mono=False)[0], sr, sec), sr, sec\n",
    "                ),\n",
    "                filenames,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    mixed = np.array(list(map(lambda f: librosa.to_mono(f), src1_src2)))\n",
    "    src1, src2 = src1_src2[:, 0], src1_src2[:, 1]\n",
    "    return mixed, src1, src2\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def to_spectrogram(wav, len_frame=ModelConfig.L_FRAME, len_hop=ModelConfig.L_HOP):\n",
    "    return np.array(\n",
    "        list(map(lambda w: librosa.stft(w, n_fft=len_frame, hop_length=len_hop), wav))\n",
    "    )\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def to_wav(mag, phase, len_hop=ModelConfig.L_HOP):\n",
    "    stft_matrix = get_stft_matrix(mag, phase)\n",
    "    return np.array(\n",
    "        list(map(lambda s: librosa.istft(s, hop_length=len_hop), stft_matrix))\n",
    "    )\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def to_wav_from_spec(stft_maxrix, len_hop=ModelConfig.L_HOP):\n",
    "    return np.array(\n",
    "        list(map(lambda s: librosa.istft(s, hop_length=len_hop), stft_maxrix))\n",
    "    )\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def to_wav_mag_only(\n",
    "    mag,\n",
    "    init_phase,\n",
    "    len_frame=ModelConfig.L_FRAME,\n",
    "    len_hop=ModelConfig.L_HOP,\n",
    "    num_iters=50,\n",
    "):\n",
    "    # return np.array(list(map(lambda m_p: griffin_lim(m, len_frame, len_hop, num_iters=num_iters, phase_angle=p)[0], list(zip(mag, init_phase))[1])))\n",
    "    return np.array(\n",
    "        list(\n",
    "            map(\n",
    "                lambda m: lambda p: griffin_lim(\n",
    "                    m, len_frame, len_hop, num_iters=num_iters, phase_angle=p\n",
    "                ),\n",
    "                list(zip(mag, init_phase))[1],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def get_magnitude(stft_matrixes):\n",
    "    return np.abs(stft_matrixes)\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def get_phase(stft_maxtrixes):\n",
    "    return np.angle(stft_maxtrixes)\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def get_stft_matrix(magnitudes, phases):\n",
    "    return magnitudes * np.exp(1.0j * phases)\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def soft_time_freq_mask(target_src, remaining_src):\n",
    "    mask = np.abs(target_src) / (\n",
    "        np.abs(target_src) + np.abs(remaining_src) + np.finfo(float).eps\n",
    "    )\n",
    "    return mask\n",
    "\n",
    "\n",
    "# Batch considered\n",
    "def hard_time_freq_mask(target_src, remaining_src):\n",
    "    mask = np.where(target_src > remaining_src, 1.0, 0.0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def write_wav(data, path, sr=ModelConfig.SR, format=\"wav\", subtype=\"PCM_16\"):\n",
    "    sf.write(\"{}.wav\".format(path), data, sr, format=format, subtype=subtype)\n",
    "\n",
    "\n",
    "def griffin_lim(mag, len_frame, len_hop, num_iters, phase_angle=None, length=None):\n",
    "    assert num_iters > 0\n",
    "    if phase_angle is None:\n",
    "        phase_angle = np.pi * np.random.rand(*mag.shape)\n",
    "    spec = get_stft_matrix(mag, phase_angle)\n",
    "    for i in range(num_iters):\n",
    "        wav = librosa.istft(\n",
    "            spec, win_length=len_frame, hop_length=len_hop, length=length\n",
    "        )\n",
    "        if i != num_iters - 1:\n",
    "            spec = librosa.stft(\n",
    "                wav, n_fft=len_frame, win_length=len_frame, hop_length=len_hop\n",
    "            )\n",
    "            _, phase = librosa.magphase(spec)\n",
    "            phase_angle = np.angle(phase)\n",
    "            spec = get_stft_matrix(mag, phase_angle)\n",
    "    return wav\n",
    "\n",
    "\n",
    "def _pad_wav(wav, sr, duration):\n",
    "    assert wav.ndim <= 2\n",
    "\n",
    "    n_samples = int(sr * duration)\n",
    "    pad_len = np.maximum(0, n_samples - wav.shape[-1])\n",
    "    if wav.ndim == 1:\n",
    "        pad_width = (0, pad_len)\n",
    "    else:\n",
    "        pad_width = ((0, 0), (0, pad_len))\n",
    "    wav = np.pad(wav, pad_width=pad_width, mode=\"constant\", constant_values=0)\n",
    "\n",
    "    return wav\n",
    "\n",
    "\n",
    "def _sample_range(wav, sr, duration):\n",
    "    assert wav.ndim <= 2\n",
    "\n",
    "    target_len = int(sr * duration)\n",
    "    wav_len = wav.shape[-1]\n",
    "    start = np.random.choice(range(np.maximum(1, wav_len - target_len)), 1)[0]\n",
    "    end = start + target_len\n",
    "    if wav.ndim == 1:\n",
    "        wav = wav[start:end]\n",
    "    else:\n",
    "        wav = wav[:, start:end]\n",
    "    return wav\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from os import walk\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def next_wavs(self, sec, size=1):\n",
    "        wavfiles = []\n",
    "        for (root, dirs, files) in walk(self.path):\n",
    "            wavfiles.extend(\n",
    "                [\"{}/{}\".format(root, f) for f in files if f.endswith(\".wav\")]\n",
    "            )\n",
    "        wavfiles = random.sample(wavfiles, size)\n",
    "        mixed, src1, src2 = get_random_wav(wavfiles, sec, ModelConfig.SR)\n",
    "        return mixed, src1, src2, wavfiles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import matplotlib as plt\n",
    "import librosa.display\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Model\n",
    "    model = Model()\n",
    "\n",
    "    # Loss, Optimizer\n",
    "    global_step = tf.Variable(\n",
    "        0, dtype=tf.int32, trainable=False, name=\"global_step\")\n",
    "    loss_fn = model.loss()\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=TrainConfig.LR).minimize(\n",
    "        loss_fn, global_step=global_step\n",
    "    )\n",
    "\n",
    "    # Summaries\n",
    "    summary_op = summaries(model, loss_fn)\n",
    "\n",
    "    with tf.Session(config=TrainConfig.session_conf) as sess:\n",
    "\n",
    "        # Initialized, Load state\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        model.load_state(sess, TrainConfig.CKPT_PATH)\n",
    "\n",
    "        writer = tf.summary.FileWriter(TrainConfig.GRAPH_PATH, sess.graph)\n",
    "\n",
    "        # Input source\n",
    "        data = Data(TrainConfig.DATA_PATH)\n",
    "\n",
    "        loss = Diff()\n",
    "        for step in range(\n",
    "            global_step.eval(), TrainConfig.FINAL_STEP\n",
    "        ):  # changed xrange to range for py3\n",
    "            mixed_wav, src1_wav, src2_wav, _ = data.next_wavs(\n",
    "                TrainConfig.SECONDS, TrainConfig.NUM_WAVFILE\n",
    "            )\n",
    "\n",
    "            mixed_spec = to_spectrogram(mixed_wav)\n",
    "            mixed_mag = get_magnitude(mixed_spec)\n",
    "\n",
    "            src1_spec, src2_spec = to_spectrogram(\n",
    "                src1_wav), to_spectrogram(src2_wav)\n",
    "            src1_mag, src2_mag = get_magnitude(\n",
    "                src1_spec), get_magnitude(src2_spec)\n",
    "\n",
    "            src1_batch, _ = model.spec_to_batch(src1_mag)\n",
    "            src2_batch, _ = model.spec_to_batch(src2_mag)\n",
    "            mixed_batch, _ = model.spec_to_batch(mixed_mag)\n",
    "\n",
    "            l, _, summary = sess.run(\n",
    "                [loss_fn, optimizer, summary_op],\n",
    "                feed_dict={\n",
    "                    model.x_mixed: mixed_batch,\n",
    "                    model.y_src1: src1_batch,\n",
    "                    model.y_src2: src2_batch,\n",
    "                },\n",
    "            )\n",
    "\n",
    "            loss.update(l)\n",
    "            print(\n",
    "                \"step-{}\\td_loss={:2.2f}\\tloss={}\".format(\n",
    "                    step, loss.diff * 100, loss.value\n",
    "                )\n",
    "            )\n",
    "\n",
    "            writer.add_summary(summary, global_step=step)\n",
    "\n",
    "            # Save state\n",
    "            if step % TrainConfig.CKPT_STEP == 0:\n",
    "                tf.train.Saver().save(\n",
    "                    sess, TrainConfig.CKPT_PATH + \"/checkpoint\", global_step=step\n",
    "                )\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "def summaries(model, loss):\n",
    "    for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "        tf.summary.histogram(v.name, v)\n",
    "        tf.summary.histogram(\"grad/\" + v.name, tf.gradients(loss, v))\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.histogram(\"x_mixed\", model.x_mixed)\n",
    "    tf.summary.histogram(\"y_src1\", model.y_src1)\n",
    "    tf.summary.histogram(\"y_src2\", model.y_src1)\n",
    "    return tf.summary.merge_all()\n",
    "\n",
    "\n",
    "def setup_path():\n",
    "    if TrainConfig.RE_TRAIN:\n",
    "        if os.path.exists(TrainConfig.CKPT_PATH):\n",
    "            shutil.rmtree(TrainConfig.CKPT_PATH)\n",
    "        if os.path.exists(TrainConfig.GRAPH_PATH):\n",
    "            shutil.rmtree(TrainConfig.GRAPH_PATH)\n",
    "    if not os.path.exists(TrainConfig.CKPT_PATH):\n",
    "        os.makedirs(TrainConfig.CKPT_PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_path()\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from mir_eval.separation import bss_eval_sources\n",
    "\n",
    "\n",
    "\n",
    "def eval():\n",
    "    # Model\n",
    "    model = Model()\n",
    "    global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name=\"global_step\")\n",
    "\n",
    "    with tf.Session(config=EvalConfig.session_conf) as sess:\n",
    "\n",
    "        # Initialized, Load state\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        model.load_state(sess, EvalConfig.CKPT_PATH)\n",
    "\n",
    "        writer = tf.summary.FileWriter(EvalConfig.GRAPH_PATH, sess.graph)\n",
    "\n",
    "        data = Data(EvalConfig.DATA_PATH)\n",
    "        mixed_wav, src1_wav, src2_wav, wavfiles = data.next_wavs(\n",
    "            EvalConfig.SECONDS, EvalConfig.NUM_EVAL\n",
    "        )\n",
    "\n",
    "        mixed_spec = to_spectrogram(mixed_wav)\n",
    "        mixed_mag = get_magnitude(mixed_spec)\n",
    "        mixed_batch, padded_mixed_mag = model.spec_to_batch(mixed_mag)\n",
    "        mixed_phase = get_phase(mixed_spec)\n",
    "\n",
    "        assert np.all(\n",
    "            np.equal(\n",
    "                model.batch_to_spec(mixed_batch, EvalConfig.NUM_EVAL), padded_mixed_mag\n",
    "            )\n",
    "        )\n",
    "\n",
    "        (pred_src1_mag, pred_src2_mag) = sess.run(\n",
    "            model(), feed_dict={model.x_mixed: mixed_batch}\n",
    "        )\n",
    "\n",
    "        seq_len = mixed_phase.shape[-1]\n",
    "        pred_src1_mag = model.batch_to_spec(pred_src1_mag, EvalConfig.NUM_EVAL)[\n",
    "            :, :, :seq_len\n",
    "        ]\n",
    "        pred_src2_mag = model.batch_to_spec(pred_src2_mag, EvalConfig.NUM_EVAL)[\n",
    "            :, :, :seq_len\n",
    "        ]\n",
    "\n",
    "        # Time-frequency masking\n",
    "        mask_src1 = soft_time_freq_mask(pred_src1_mag, pred_src2_mag)\n",
    "        # mask_src1 = hard_time_freq_mask(pred_src1_mag, pred_src2_mag)\n",
    "        mask_src2 = 1.0 - mask_src1\n",
    "        pred_src1_mag = mixed_mag * mask_src1\n",
    "        pred_src2_mag = mixed_mag * mask_src2\n",
    "\n",
    "        # (magnitude, phase) -> spectrogram -> wav\n",
    "        if EvalConfig.GRIFFIN_LIM:\n",
    "            pred_src1_wav = to_wav_mag_only(\n",
    "                pred_src1_mag,\n",
    "                init_phase=mixed_phase,\n",
    "                num_iters=EvalConfig.GRIFFIN_LIM_ITER,\n",
    "            )\n",
    "            pred_src2_wav = to_wav_mag_only(\n",
    "                pred_src2_mag,\n",
    "                init_phase=mixed_phase,\n",
    "                num_iters=EvalConfig.GRIFFIN_LIM_ITER,\n",
    "            )\n",
    "        else:\n",
    "            pred_src1_wav = to_wav(pred_src1_mag, mixed_phase)\n",
    "            pred_src2_wav = to_wav(pred_src2_mag, mixed_phase)\n",
    "\n",
    "        # Write the result\n",
    "        tf.summary.audio(\n",
    "            \"GT_mixed\", mixed_wav, ModelConfig.SR, max_outputs=EvalConfig.NUM_EVAL\n",
    "        )\n",
    "        tf.summary.audio(\n",
    "            \"Pred_music\", pred_src1_wav, ModelConfig.SR, max_outputs=EvalConfig.NUM_EVAL\n",
    "        )\n",
    "        tf.summary.audio(\n",
    "            \"Pred_vocal\", pred_src2_wav, ModelConfig.SR, max_outputs=EvalConfig.NUM_EVAL\n",
    "        )\n",
    "\n",
    "        if EvalConfig.EVAL_METRIC:\n",
    "            # Compute BSS metrics\n",
    "            gnsdr, gsir, gsar = bss_eval_global(\n",
    "                mixed_wav, src1_wav, src2_wav, pred_src1_wav, pred_src2_wav\n",
    "            )\n",
    "\n",
    "            # Write the score of BSS metrics\n",
    "            tf.summary.scalar(\"GNSDR_music\", gnsdr[0])\n",
    "            tf.summary.scalar(\"GSIR_music\", gsir[0])\n",
    "            tf.summary.scalar(\"GSAR_music\", gsar[0])\n",
    "            tf.summary.scalar(\"GNSDR_vocal\", gnsdr[1])\n",
    "            tf.summary.scalar(\"GSIR_vocal\", gsir[1])\n",
    "            tf.summary.scalar(\"GSAR_vocal\", gsar[1])\n",
    "\n",
    "        if EvalConfig.WRITE_RESULT:\n",
    "            # Write the result\n",
    "            for i in range(len(wavfiles)):\n",
    "                name = wavfiles[i].replace(\"/\", \"-\").replace(\".wav\", \"\")\n",
    "                write_wav(\n",
    "                    mixed_wav[i],\n",
    "                    \"{}/{}-{}\".format(EvalConfig.RESULT_PATH, name, \"original\"),\n",
    "                )\n",
    "                write_wav(\n",
    "                    pred_src1_wav[i],\n",
    "                    \"{}/{}-{}\".format(EvalConfig.RESULT_PATH, name, \"music\"),\n",
    "                )\n",
    "                write_wav(\n",
    "                    pred_src2_wav[i],\n",
    "                    \"{}/{}-{}\".format(EvalConfig.RESULT_PATH, name, \"voice\"),\n",
    "                )\n",
    "\n",
    "        writer.add_summary(\n",
    "            sess.run(tf.summary.merge_all()), global_step=global_step.eval()\n",
    "        )\n",
    "\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "def bss_eval_global(mixed_wav, src1_wav, src2_wav, pred_src1_wav, pred_src2_wav):\n",
    "    len_cropped = pred_src1_wav.shape[-1]\n",
    "    src1_wav = src1_wav[:, :len_cropped]\n",
    "    src2_wav = src2_wav[:, :len_cropped]\n",
    "    mixed_wav = mixed_wav[:, :len_cropped]\n",
    "    gnsdr = gsir = gsar = np.zeros(2)\n",
    "    total_len = 0\n",
    "    for i in range(EvalConfig.NUM_EVAL):\n",
    "        sdr, sir, sar, _ = bss_eval_sources(\n",
    "            np.array([src1_wav[i], src2_wav[i]]),\n",
    "            np.array([pred_src1_wav[i], pred_src2_wav[i]]),\n",
    "            False,\n",
    "        )\n",
    "        sdr_mixed, _, _, _ = bss_eval_sources(\n",
    "            np.array([src1_wav[i], src2_wav[i]]),\n",
    "            np.array([mixed_wav[i], mixed_wav[i]]),\n",
    "            False,\n",
    "        )\n",
    "        nsdr = sdr - sdr_mixed\n",
    "        gnsdr += len_cropped * nsdr\n",
    "        gsir += len_cropped * sir\n",
    "        gsar += len_cropped * sar\n",
    "        total_len += len_cropped\n",
    "    gnsdr = gnsdr / total_len\n",
    "    gsir = gsir / total_len\n",
    "    gsar = gsar / total_len\n",
    "    return gnsdr, gsir, gsar\n",
    "\n",
    "\n",
    "def setup_path():\n",
    "    if EvalConfig.RE_EVAL:\n",
    "        if os.path.exists(EvalConfig.GRAPH_PATH):\n",
    "            shutil.rmtree(EvalConfig.GRAPH_PATH)\n",
    "        if os.path.exists(EvalConfig.RESULT_PATH):\n",
    "            shutil.rmtree(EvalConfig.RESULT_PATH)\n",
    "\n",
    "    if not os.path.exists(EvalConfig.RESULT_PATH):\n",
    "        os.makedirs(EvalConfig.RESULT_PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_path()\n",
    "    eval()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
